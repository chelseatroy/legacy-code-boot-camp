{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're working on a test framework to write tests for the below (bad) implementation of `find_twos`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_twos(first_list, second_list):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have written some matchers for our test framework: `has_items`, `has_size`, `is_empty`, and `equals`. For convenience, I stuck our implementations into a file called `matchers` inside a package called `phoenix_test` _right here in this folder_ (you can even go look at it!)\n",
    "\n",
    "This next line imports all that stuff we wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix_test.matchers import FailedAssertion, Assertion, assert_that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used that stuff to write some tests for our mediocre-at-best implementation of `find_twos` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def test_empty_inputs():\n",
    "        print(\"\")\n",
    "        assert_that(find_twos(\"\", \"\")).is_empty()\n",
    "        assert_that(find_twos(\"2\", \"\")).is_empty()\n",
    "        assert_that(find_twos(\"2\", \"\")).is_empty()\n",
    "\n",
    "    def test_non_matching_sets():\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).is_empty()\n",
    "\n",
    "    def test_non_matching_twos():\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).is_empty()\n",
    "        \n",
    "    def test_matches():\n",
    "        # This test is going to fail because our find_twos method works crappy\n",
    "        # This is by design to make sure our testing framework...actually tests something\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).has_size(1)\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).has_size(2)\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).has_items(2, 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we run the above block of code?\n",
    "\n",
    "Ah, yes, nothing. The reason nothing happens is that the above block of code _defines_ four test methods, but it doesn't _call_ them. In order to run these tests, we have to call our test methods like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FailedAssertion",
     "evalue": "Expected [] to have size 1 but it instead had size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedAssertion\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-855a1cbb8ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_non_matching_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_non_matching_twos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-1c41ac8fc15a>\u001b[0m in \u001b[0;36mtest_matches\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# This test is going to fail because our find_twos method works crappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# This is by design to make sure our testing framework...actually tests something\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0massert_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_twos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"12\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2, 12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0massert_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_twos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"12\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2, 12\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0massert_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_twos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1, 2, 20, 22, 44, 99\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3, 5, 22, 100, 44, 2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/MPCS-51042-46/materials/test_framework_exercise/phoenix_test/matchers.py\u001b[0m in \u001b[0;36mhas_size\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFailedAssertion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected {self.expression} to have size {size} but it instead had size {len(self.expression)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhas_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedAssertion\u001b[0m: Expected [] to have size 1 but it instead had size 0"
     ]
    }
   ],
   "source": [
    "test_empty_inputs()\n",
    "test_non_matching_sets()\n",
    "test_non_matching_twos()\n",
    "test_matches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's kind of annoying. We don't want to have to explicitly _call_ every test that we write like this. It would be annoying to have to keep a file that just lists out and calls every single test method so we can run our tests all at once...plus, that's mistake-prone, since we might write a test and forget to put it in the list, or delete one and forget to take it out.\n",
    "\n",
    "What if our `Test` superclass were able to run all our tests for us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    # Runs all the test methods. HOW?!?!\n",
    "    def run(self):\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            print(f\"Running {method}.\")\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "            except Exception as e:\n",
    "                print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at what is happening in this `run` method.\n",
    "\n",
    "Practice making a referential map by tracing the `Test` class. Which Python functionality is it using? How do those functions work? Do you have any open questions?\n",
    "\n",
    "\n",
    "### A Tool for You: Python's Built-In Documentation\n",
    "\n",
    "Python provides you with some assistance for researching code that you are reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dir([object]) -> list of strings\\n\\nIf called without an argument, return the names in the current scope.\\nElse, return an alphabetized list of names comprising (some of) the attributes\\nof the given object, and of attributes reachable from it.\\nIf the object supplies a method named __dir__, it will be used; otherwise\\nthe default dir() logic is used and returns:\\n  for a module object: the module's attributes.\\n  for a class object:  its attributes, and recursively the attributes\\n    of its bases.\\n  for any other object: its attributes, its class's attributes, and\\n    recursively the attributes of its class's base classes.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function dir in module builtins:\n",
      "\n",
      "dir(...)\n",
      "    dir([object]) -> list of strings\n",
      "    \n",
      "    If called without an argument, return the names in the current scope.\n",
      "    Else, return an alphabetized list of names comprising (some of) the attributes\n",
      "    of the given object, and of attributes reachable from it.\n",
      "    If the object supplies a method named __dir__, it will be used; otherwise\n",
      "    the default dir() logic is used and returns:\n",
      "      for a module object: the module's attributes.\n",
      "      for a class object:  its attributes, and recursively the attributes\n",
      "        of its bases.\n",
      "      for any other object: its attributes, its class's attributes, and\n",
      "        recursively the attributes of its class's base classes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Test class is going to be our **superclass**. We can now **subclass** that Test class like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindTwosTest(Test):\n",
    "    test_useless_attribute = None\n",
    "    test_other_useless_attribute = None\n",
    "\n",
    "    def test_empty_inputs(self):\n",
    "        print(\"\")\n",
    "        assert_that(find_twos(\"\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "\n",
    "    def test_non_matching_sets(self):\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).equals([])\n",
    "\n",
    "    def test_non_matching_twos(self):\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).equals([])\n",
    "        \n",
    "    def test_matches(self):\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).equals([2, 22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test_empty_inputs.\n",
      "\n",
      "Running test_matches.\n",
      "Expected [12] but got []\n",
      "Running test_non_matching_sets.\n",
      "Running test_non_matching_twos.\n"
     ]
    }
   ],
   "source": [
    "FindTwosTest().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Tool for You: Running Your Own Experiments\n",
    "\n",
    "On _this particular_ code, I have kept it in small chunks inside of a REPL environment so that you can remove or change lines of code to investigate what they are doing.\n",
    "\n",
    "### Challenge: \n",
    "\n",
    "1. What happens if you comment out `and callable(getattr(self.__class__, token))` in the list comprehension?\n",
    "1. What happens if you remove the try/except above and just call `getattr(self.__class__, method).__call__(self)` right after the print statement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a test runner. Woo!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
