{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a test runner that kinda works. Buuut....\n",
    "\n",
    "- What if we have several different subclasses of Test and we want to run all the tests at once?\n",
    "- What if we have some code that we need to run before and after every test?\n",
    "- What if we want to be able to skip a test, or only run specific tests according to some rule we made up?\n",
    "\n",
    "It is time for us to make these dreams come to life. But remember! We want to keep our implementation as _streamlined_ as possible. **Let's note each time we find ourselves making a streamlining choice in these exercises.**\n",
    "\n",
    "First, we import the matchers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix_test.matchers import FailedAssertion, Assertion, assert_that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import colorama, so the test runner can print in colors. Who doesn't love colors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install colorama \n",
    "\n",
    "from colorama import Fore, Back, Style "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, here's where we're starting on the test runner today:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    # Runs all the test methods. HOW?!?!\n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"{method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"{method}:  {e}\") \n",
    "        print(Style.RESET_ALL)\n",
    "        print(f\"{pass_count} out of {run_count} tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use this runner on some example tests, we get some beautiful test output. Here, give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_twos(one, two):\n",
    "    return []\n",
    "\n",
    "class FindTwosTest(Test):\n",
    "\n",
    "    def test_empty_inputs(self):\n",
    "        assert_that(find_twos(\"\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "\n",
    "    def test_non_matching_sets(self):\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).equals([])\n",
    "\n",
    "    def test_non_matching_twos(self):\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).equals([])\n",
    "        \n",
    "    def test_matches(self):\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).equals([2, 22])\n",
    "        \n",
    "FindTwosTest().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is where we are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "What if I have several different subclasses of Test and I want to run all my tests at once? So, in addition to my `FindTwosTest`, I also want to run this spiffy test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortedTests(Test):\n",
    "    def test_sort_integers(self):\n",
    "        assert_that(sorted([3, 1, 2])).equals([1, 2, 3])\n",
    "\n",
    "    def test_sort_strings(self):\n",
    "        assert_that(sorted([\"C\", \"A\", \"B\"])).equals([\"A\", \"B\", \"C\"])\n",
    "\n",
    "SortedTests().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: make this statement run all the tests:\n",
    "\n",
    "`Test.run_all()`\n",
    "\n",
    "Such that, whichever classes have subclassed `Test`, they all run when you run that command. \n",
    "\n",
    "Hint: use `__init_subclass__`. This method is run on a superclass every time a subclass gets instantiated. You can use it to register all of the subclasses in a collection at the superclass level. You can read all about exactly how this method works in [the PEP proposal](https://www.python.org/dev/peps/pep-0487/) that introduced it to Python!\n",
    "\n",
    "Here is our Test class so far with some starter code added for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    types = []\n",
    "    \n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        #register subtypes with the parent class\n",
    "    \n",
    "    @classmethod\n",
    "    def run_all(cls):\n",
    "        pass\n",
    "        #what goes here? \n",
    "        \n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"    {method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"    {method}:  {e}\") \n",
    "        print(Style.RESET_ALL + f\"    {pass_count} out of {run_count} tests passed.\\n\")\n",
    "        return pass_count, run_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all our test classes together, so you can easily check whether all of your test methods are getting run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindTwosTest(Test):\n",
    "\n",
    "    def test_empty_inputs(self):\n",
    "        assert_that(find_twos(\"\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "\n",
    "    def test_non_matching_sets(self):\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).equals([])\n",
    "\n",
    "    def test_non_matching_twos(self):\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).equals([])\n",
    "        \n",
    "    def test_matches(self):\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).equals([2, 22])\n",
    "        \n",
    "class SortedTests(Test):\n",
    "    def test_sort_integers(self):\n",
    "        assert_that(sorted([3, 1, 2])).equals([1, 2, 3])\n",
    "\n",
    "    def test_sort_strings(self):\n",
    "        assert_that(sorted([\"C\", \"A\", \"B\"])).equals([\"A\", \"B\", \"C\"])\n",
    "\n",
    "Test.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: \n",
    "\n",
    "1. Make the test suite print the name of each test class before running it\n",
    "2. Make the test suite print a total number of tests run and passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: \n",
    "\n",
    "What if I have some code that I need to run before and after every test? Suppose, for example, that I am keeping a cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedStringQueue:\n",
    "    items = []\n",
    "    \n",
    "    @classmethod\n",
    "    def validate(cls):\n",
    "        invalids = 0\n",
    "        for item in CachedStringQueue.items:\n",
    "            if item == 'invalid':\n",
    "                invalids += 1\n",
    "        cls.items = sorted(cls.items)[invalids:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I have two test suites for the cache: one that checks how to put things in it (assuming an empty cache) and one that checks how to invalidate items (which requires some stuff to be in the cache.)\n",
    "\n",
    "Perhaps we could include `setup` and `teardown` methods in our test class that run before and after each separate test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheSetAndFetchTests(Test):\n",
    "    def test_set_and_fetch(self):\n",
    "        CachedStringQueue.items.append(\"stringo\")\n",
    "        assert_that(CachedStringQueue.items.pop(0)).equals(\"stringo\")\n",
    "    \n",
    "class CacheValidationTests(Test):\n",
    "    def setup(self):\n",
    "        print(\"SETUP RUN\")\n",
    "        CachedStringQueue.items = ['valid', 'invalid', 'valid']\n",
    "    \n",
    "    def teardown(self):\n",
    "        print(\"TEARDOWN RUN\")\n",
    "        CachedStringQueue.items = []\n",
    "        \n",
    "    def test_validation(self):\n",
    "        CachedStringQueue.validate()\n",
    "        assert_that(CachedStringQueue.items).has_size(2)\n",
    "        assert_that(set(CachedStringQueue.items)).equals({'valid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CacheSetAndFetchTests().run()\n",
    "CacheValidationTests().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge:\n",
    "\n",
    "Add this `setup` and `teardown` function to our test functionality. Remember to call those methods in the test runner to run before and after each test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    types = []\n",
    "    \n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        cls.types.append(cls)\n",
    "    \n",
    "    @classmethod\n",
    "    def run_all(cls):\n",
    "        pass_count = 0\n",
    "        run_count = 0\n",
    "        \n",
    "        for typ in cls.types:\n",
    "            print(f\"Running {typ.__name__}: \")\n",
    "            passed, runned = typ().run() \n",
    "            pass_count += passed\n",
    "            run_count += runned\n",
    "        \n",
    "        print(f\"{pass_count} out of {run_count} tests passed.\\n\")\n",
    "    \n",
    "        \n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "            if token.startswith(\"test\")  \\\n",
    "            and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"    {method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"    {method}:  {e}\") \n",
    "        print(Style.RESET_ALL + f\"    {pass_count} out of {run_count} tests passed.\\n\")\n",
    "        return pass_count, run_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CacheSetAndFetchTests().run()\n",
    "CacheValidationTests().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: \n",
    "\n",
    "What if I want to be able to only run specific tests according to some rule I made up?\n",
    "\n",
    "Maybe:\n",
    "\n",
    "`Test.run_all(only=\"match\")` only runs tests with \"match\" in their name. You can modify this version of the test runner to make that happen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    types = []\n",
    "    \n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        cls.types.append(cls)\n",
    "    \n",
    "    @classmethod\n",
    "    def run_all(cls):\n",
    "        pass_count = 0\n",
    "        run_count = 0\n",
    "        \n",
    "        for typ in cls.types:\n",
    "            print(f\"Running {typ.__name__}: \")\n",
    "            passed, runned = typ().run() \n",
    "            pass_count += passed\n",
    "            run_count += runned\n",
    "        \n",
    "        print(f\"{pass_count} out of {run_count} tests passed.\\n\")\n",
    "    \n",
    "    def setup(self):\n",
    "        pass\n",
    "    \n",
    "    def teardown(self):\n",
    "        pass\n",
    "        \n",
    "    def run(self):\n",
    "        run_count = 0\n",
    "        pass_count = 0\n",
    "        test_methods = [\n",
    "            token for token in dir(self) \\\n",
    "                if token.startswith(\"test\")  \\\n",
    "                and callable(getattr(self.__class__, token))\n",
    "        ]\n",
    "        for method in test_methods:\n",
    "            self.setup()\n",
    "            run_count += 1\n",
    "            try:\n",
    "                getattr(self.__class__, method).__call__(self)\n",
    "                pass_count += 1\n",
    "                print(Fore.GREEN + f\"    {method} passed!\")\n",
    "            except Exception as e:\n",
    "                print(Fore.RED +f\"    {method}:  {e}\") \n",
    "            self.teardown()\n",
    "        print(Style.RESET_ALL + f\"    {pass_count} out of {run_count} tests passed.\\n\")\n",
    "        return pass_count, run_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we'll _just_ run `FindTwosTest` to keep our example small while making sure that our code works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindTwosTest(Test):\n",
    "\n",
    "    def test_empty_inputs(self):\n",
    "        assert_that(find_twos(\"\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "        assert_that(find_twos(\"2\", \"\")).equals([])\n",
    "\n",
    "    def test_non_matching_sets(self):\n",
    "        assert_that(find_twos(\"1\", \"1, 3\")).equals([])\n",
    "\n",
    "    def test_non_matching_twos(self):\n",
    "        assert_that(find_twos(\"2\", \"1, 3\")).equals([])\n",
    "        \n",
    "    def test_matches(self):\n",
    "        assert_that(find_twos(\"12\", \"2, 12\")).equals([12])\n",
    "        assert_that(find_twos(\"1, 2, 20, 22, 44, 99\", \"3, 5, 22, 100, 44, 2\")).equals([2, 22])\n",
    "\n",
    "Test.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.run_all(only=\"match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
